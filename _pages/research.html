---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---

<article>
<div class="cell">
<table>
<tbody>
<tr>
<td>
	<b>Improving BERT Fine-Tuning via Self-Ensemble and Self-Distillation</b>
	<br>
	Supervisor:  <a href="https://xpqiu.github.io/">Xipeng Qiu</a> and <a href="http://xuyige.github.io/">Yige Xu</a>.
	<br>
	Inspired by the success of widely-used ensemble models, we propose a self-ensemble method combining knowledge of BERT models at different time steps for more robust and accurate results, and further introduce a self-distillation idea to enable the base model to learn from its self-ensemble.
</td>
<td width="50%">
	<img class="side" src="../images/self-distillation.png" width="100%">
</td>
</tr>
</tbody>
</table>
</div>
</article>
